{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce163b9-595e-474e-99c1-00cafd3d55f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"10\" \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"10\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"10\" \n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "# Load configuration\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"./config.ini\")\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataloaders.dataloader_robotcar import RobotCarDataset, transform_grd, transform_sat\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "from utils.utils import weighted_procrustes_2d, desc_l2norm\n",
    "from models.loss import loss_bev_space, compute_infonce_loss\n",
    "from models.model_robotcar_eval import CVM\n",
    "from models.modules import DinoExtractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "beta = 10.0\n",
    "epoch = 98\n",
    "num_samples_matches4vis = 50\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility from config\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(mode=True, warn_only=True)\n",
    "\n",
    "set_seeds(config.getint(\"RandomSeed\", \"seed\"))\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"./config.ini\")\n",
    "\n",
    "# Load hyperparameters from config\n",
    "dataset = config[\"Dataset\"][\"dataset\"]\n",
    "\n",
    "# Dataset depend hyperparameters\n",
    "dataset_root = config[\"RobotCar\"][\"local_dataset_root\"]\n",
    "raw_ground_image_size = ast.literal_eval(config.get(\"RobotCar\", \"raw_ground_image_size\"))\n",
    "cropped_ground_image_size = ast.literal_eval(config.get(\"RobotCar\", \"cropped_ground_image_size\"))\n",
    "ground_image_size = ast.literal_eval(config.get(\"RobotCar\", \"ground_image_size\"))\n",
    "satellite_image_size = ast.literal_eval(config.get(\"RobotCar\", \"satellite_image_size\"))\n",
    "\n",
    "grid_size_h = config.getfloat(\"RobotCar\", \"grid_size_h\")\n",
    "grid_size_v = config.getfloat(\"RobotCar\", \"grid_size_v\")\n",
    "\n",
    "learning_rate = config.getfloat(\"Training\", \"learning_rate\")\n",
    "\n",
    "grd_bev_res = config.getint(\"Model\", \"grd_bev_res\")\n",
    "grd_height_res = config.getint(\"Model\", \"grd_height_res\")\n",
    "sat_bev_res = config.getint(\"Model\", \"sat_bev_res\")\n",
    "\n",
    "num_keypoints = config.getint(\"Model\", \"num_keypoints\")\n",
    "\n",
    "num_samples_matches = config.getint(\"Matching\", \"num_samples_matches\")\n",
    "\n",
    "loss_grid_size = config.getfloat(\"Loss\", \"loss_grid_size\")\n",
    "num_virtual_point = config.getint(\"Loss\", \"num_virtual_point\")\n",
    "\n",
    "label = (f\"Robotcar_num_matches_{num_samples_matches}\"\n",
    "         f\"_beta_{beta}_grd_bev_res_{grd_bev_res}_height_res_{grd_height_res}\"\n",
    "         f\"_sat_res_{sat_bev_res}_loss_grid_{loss_grid_size}\"\n",
    "         f\"_h_{int(grid_size_h)}_v_{grid_size_v}_lr_{learning_rate}\")\n",
    "\n",
    "print(f\"Experiment label: {label}\")\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "training_set = RobotCarDataset(\n",
    "    root=dataset_root, split='train',\n",
    "    transform=(transform_grd, transform_sat)\n",
    ")\n",
    "\n",
    "val_set = RobotCarDataset(\n",
    "    root=dataset_root, split='val',\n",
    "    transform=(transform_grd, transform_sat)\n",
    ")\n",
    "\n",
    "test_set = RobotCarDataset(\n",
    "    root=dataset_root, split='test',\n",
    "    transform=(transform_grd, transform_sat)\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize shared feature extractor\n",
    "shared_feature_extractor = DinoExtractor().to(device)\n",
    "\n",
    "# Initialize CVM Model\n",
    "CVM_model = CVM(device, grd_bev_res=grd_bev_res, grd_height_res=grd_height_res, \n",
    "                sat_bev_res=sat_bev_res, num_keypoints=num_keypoints, \n",
    "                embed_dim=1024, grid_size_h=grid_size_h, grid_size_v=grid_size_v)\n",
    "\n",
    "model_path = f'/home/ziminxia/Work/scitas_mount/checkpoints/{label}/{epoch}/model.pt'\n",
    "CVM_model.load_state_dict(torch.load(model_path))\n",
    "CVM_model.to(device)\n",
    "CVM_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a13a3-3ce1-459a-b84a-c2ff5097a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_metric_grid(grid_size, res, batch_size):\n",
    "    x, y = np.linspace(-grid_size/2, grid_size/2, res[0]), np.linspace(-grid_size/2, grid_size/2, res[1])\n",
    "    metric_x, metric_y = np.meshgrid(x, y, indexing='ij')\n",
    "    metric_x, metric_y = torch.tensor(metric_x).flatten().unsqueeze(0).unsqueeze(-1), torch.tensor(metric_y).flatten().unsqueeze(0).unsqueeze(-1)\n",
    "    metric_coord = torch.cat((metric_x, metric_y), -1).to(device).float()\n",
    "    return metric_coord.repeat(batch_size, 1, 1)\n",
    "\n",
    "metric_coord_grd_B = create_metric_grid(grid_size_h, (int(np.floor(grd_bev_res/2))+1, grd_bev_res), batch_size)\n",
    "metric_coord_sat_B = create_metric_grid(grid_size_h, (sat_bev_res, sat_bev_res), batch_size)\n",
    "\n",
    "x = np.linspace(-grid_size_h / 2, 0, int(np.floor(grd_bev_res/2))+1)\n",
    "y = np.linspace(-grid_size_h / 2, grid_size_h / 2, grd_bev_res)\n",
    "z = np.linspace(-grid_size_v / 2, grid_size_v / 2, grd_height_res)\n",
    "\n",
    "num_samples_matches4vis = 50\n",
    "\n",
    "for idx in range(20, 40):\n",
    "    print('idx', idx)\n",
    "    with torch.no_grad():\n",
    "        grd, sat, tgt, Rgt = test_set.__getitem__(idx)\n",
    "\n",
    "        grd, sat, tgt, Rgt = grd.unsqueeze(0).to(device), sat.unsqueeze(0).to(device), tgt.unsqueeze(0).to(device), Rgt.unsqueeze(0).to(device)\n",
    "\n",
    "        B, _, sat_size, _ = sat.shape\n",
    "        B, _, grd_H, grd_W = grd.shape\n",
    "        \n",
    "        grd_feature, sat_feature = shared_feature_extractor(grd), shared_feature_extractor(sat)\n",
    "\n",
    "        matching_score, sat_desc, grd_desc, sat_indices_topk, grd_indices_topk, matching_score_original, grd_scrs, sat_scrs, height_index = CVM_model(grd_feature, sat_feature)\n",
    "        _, num_kpts_sat, num_kpts_grd = matching_score.shape\n",
    "        \n",
    "        # Sample validation matches\n",
    "        matches_row = matching_score.flatten(1)\n",
    "        batch_idx = torch.arange(B).view(B, 1).repeat(1, num_samples_matches).reshape(B, num_samples_matches)\n",
    "        sampled_matching_idx = torch.multinomial(matches_row, num_samples_matches)\n",
    "        sampled_idx4vis = torch.argsort(matches_row, descending=True)[:,:num_samples_matches4vis]\n",
    "\n",
    "\n",
    "        sampled_matching_row = torch.div(sampled_matching_idx, num_kpts_grd, rounding_mode='trunc')\n",
    "        sampled_matching_col = sampled_matching_idx % num_kpts_grd\n",
    "\n",
    "        sampled_matching_row4vis = torch.div(sampled_idx4vis, num_kpts_grd, rounding_mode='trunc')\n",
    "        sampled_matching_col4vis = (sampled_idx4vis % num_kpts_grd)\n",
    "\n",
    "        sat_indices_sampled = torch.gather(sat_indices_topk.squeeze(1), 1, sampled_matching_row)\n",
    "        grd_indices_sampled = torch.gather(grd_indices_topk.squeeze(1), 1, sampled_matching_col)\n",
    "\n",
    "        sat_indices_sampled4vis = torch.gather(sat_indices_topk.squeeze(1), 1, sampled_matching_row4vis)\n",
    "        grd_indices_sampled4vis = torch.gather(grd_indices_topk.squeeze(1), 1, sampled_matching_col4vis)\n",
    "\n",
    "        X, Y, weights = metric_coord_sat_B[batch_idx, sat_indices_sampled, :], metric_coord_grd_B[batch_idx, grd_indices_sampled, :], matches_row[batch_idx, sampled_matching_idx]\n",
    "        R, t, ok_rank = weighted_procrustes_2d(X, Y, use_weights=True, use_mask=True, w=weights)\n",
    "\n",
    "        if t is None:\n",
    "            print('⚠️ Skipping batch: Singular transformation matrix')\n",
    "            continue\n",
    "        \n",
    "        # Compute translation error\n",
    "        t = (t / grid_size_h) * sat_size\n",
    "        translation_error = torch.norm(t - tgt, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Compute yaw error\n",
    "        Rgt_np, R_np = Rgt.cpu().numpy(), R.cpu().numpy()\n",
    "        for b in range(B):\n",
    "            cos = R_np[b,0,0]\n",
    "            sin = R_np[b,1,0]\n",
    "            yaw = np.degrees( np.arctan2(sin, cos) )            \n",
    "            \n",
    "            cos_gt = Rgt_np[b,0,0]\n",
    "            sin_gt = Rgt_np[b,1,0]\n",
    "            \n",
    "            yaw_gt = np.degrees( np.arctan2(sin_gt, cos_gt) )\n",
    "            \n",
    "            diff = np.abs(yaw - yaw_gt)\n",
    "\n",
    "            yaw_error = np.min([diff, 360-diff])         \n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.imshow(grd.squeeze(0).permute(1,2,0).cpu().numpy())       \n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.figure(figsize=(10,10))\n",
    "        # plt.imshow(sat.squeeze(0).permute(1,2,0).cpu().numpy())       \n",
    "        # plt.quiver(satellite_image_size[1]/2-tgt[0,0,1].cpu(), satellite_image_size[0]/2-tgt[0,0,0].cpu(), np.cos((90-yaw_gt )/ 180 * np.pi), np.sin((90-yaw_gt) / 180 * np.pi), facecolor='g', linewidths=0.6, scale=15, width=0.01)\n",
    "        # plt.scatter(satellite_image_size[1]/2-tgt[0,0,1].cpu(), satellite_image_size[0]/2-tgt[0,0,0].cpu(), s=300, marker='^', facecolor='g', label='GT', edgecolors='white', zorder=1)\n",
    "        # plt.quiver(satellite_image_size[1]/2-t[0,0,1].cpu(), satellite_image_size[0]/2-t[0,0,0].cpu(), np.cos((90-yaw )/ 180 * np.pi), np.sin((90-yaw) / 180 * np.pi), facecolor='gold', linewidths=0.6, scale=15, width=0.01)\n",
    "        # plt.scatter(satellite_image_size[1]/2-t[0,0,1].cpu(), satellite_image_size[0]/2-t[0,0,0].cpu(), s=300, marker='*', facecolor='gold', label='Ours', edgecolors='white', zorder=1)\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        \n",
    "        print('translation_error', translation_error)\n",
    "        print('yaw_error', yaw_error)\n",
    "\n",
    "        sat_image_plot_L = grd_H\n",
    "        grd_bev_plot_H = int(np.floor(grd_H/2))+1\n",
    "        grd_bev_plot_W = grd_H \n",
    "        \n",
    "        \n",
    "        sat_grid_size = sat_image_plot_L/sat_bev_res\n",
    "        sat_h = (torch.div(sat_indices_sampled4vis, sat_bev_res, rounding_mode='trunc')).cpu().numpy() * sat_grid_size + sat_grid_size/2\n",
    "        sat_w = (sat_indices_sampled4vis % sat_bev_res).cpu().numpy() * sat_grid_size + sat_grid_size/2\n",
    "\n",
    "        grd_bev_grid_size = grd_bev_plot_W/grd_bev_res\n",
    "        \n",
    "        grd_bev_row_indices = (torch.div(grd_indices_sampled4vis, grd_bev_res, rounding_mode='trunc')).cpu().numpy() \n",
    "        grd_bev_col_indices = (grd_indices_sampled4vis % grd_bev_res).cpu().numpy()\n",
    "        sampled_height_indices = height_index.flatten()[grd_indices_sampled4vis]\n",
    "        \n",
    "        grd_bev_h = grd_bev_row_indices * grd_bev_grid_size + grd_bev_grid_size/2\n",
    "        grd_bev_w = grd_bev_col_indices * grd_bev_grid_size + grd_bev_grid_size/2\n",
    "\n",
    "        grd_3D_sampled = np.ones([num_samples_matches4vis, 3])\n",
    "        \n",
    "        \n",
    "        for i in range(num_samples_matches4vis):\n",
    "            grd_3D_sampled[i, 0] = y[grd_bev_col_indices[0,i]]\n",
    "            grd_3D_sampled[i, 1] = -z[sampled_height_indices[0,i]]\n",
    "            grd_3D_sampled[i, 2] = -x[grd_bev_row_indices[0,i]]\n",
    "\n",
    "        \n",
    "        fx, fy = 964.828979 * ground_image_size[1] / raw_ground_image_size[1], 964.828979 * ground_image_size[0] / raw_ground_image_size[0] \n",
    "        # fx, fy = 964.828979, 964.828979  \n",
    "        cx, cy = (643.788025 - (raw_ground_image_size[1] - cropped_ground_image_size[1]) / 2) * ground_image_size[1] / cropped_ground_image_size[1], 484.407990 * ground_image_size[0] / raw_ground_image_size[0]  \n",
    "        # cx, cy = 643.788025 , 484.407990   \n",
    "        camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]\n",
    "\n",
    "        projected_points = camera_k @ np.transpose(grd_3D_sampled)\n",
    "        u = (projected_points[0, :] / projected_points[2, :] / ground_image_size[1])\n",
    "        v = (projected_points[1, :] / projected_points[2, :] / ground_image_size[0])\n",
    "        \n",
    "        \n",
    "        grd_h = v * grd_H\n",
    "        grd_w = u * grd_W\n",
    "\n",
    "        grd_points = []\n",
    "        grd_bev_points = []\n",
    "        sat_points = []\n",
    "        for i in range(num_samples_matches4vis):\n",
    "            if u[i] >= 0 and u[i] <= 1 and v[i] >= 0 and v[i] <= 1:\n",
    "                grd_bev_points.append((grd_bev_w[0, i], grd_bev_h[0, i]))\n",
    "                sat_points.append((sat_w[0, i], sat_h[0, i]))\n",
    "                grd_points.append((grd_w[i], grd_h[i]))\n",
    "\n",
    "        \n",
    "        combined_image = np.ones((grd_H, grd_W+grd_H+10, 3)) \n",
    "        grd_to_show = grd[0].permute(1,2,0).cpu().detach().numpy()\n",
    "        sat_to_show = F.interpolate(sat, size=(grd_H, grd_H), mode='bicubic', align_corners=False)[0].permute(1,2,0).cpu().detach().numpy()\n",
    "    \n",
    "        # Place the images side-by-side on the canvas\n",
    "        combined_image[:, :grd_W, :] = grd_to_show\n",
    "        combined_image[:, 10+grd_W:, :] = sat_to_show\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(15,60))\n",
    "        ax.imshow(combined_image)\n",
    "        sample_index = 0\n",
    "        for (x1, y1), (x2, y2) in zip(grd_points, sat_points):\n",
    "            # Adjust x2 coordinate for the second image\n",
    "            x2_adjusted = x2 + 10+grd_W\n",
    "            # ax.plot([x1, x2_adjusted], [y1, y2], marker='o', markersize=4, color='lime', linestyle='-', linewidth=1, alpha=sampled_matching_scores[sample_index])\n",
    "            ax.plot([x1, x2_adjusted], [y1, y2], marker='o', markersize=2, color='lime', linestyle='-', linewidth=0.8, zorder=0)\n",
    "            sample_index += 1\n",
    "        \n",
    "        plt.quiver(grd_W+sat_image_plot_L/2-tgt[0,0,1].cpu(), sat_image_plot_L/2-tgt[0,0,0].cpu(), np.cos((90-yaw_gt )/ 180 * np.pi), np.sin((90-yaw_gt) / 180 * np.pi), facecolor='g', linewidths=0.3, scale=25, width=0.006)\n",
    "        plt.scatter(grd_W+sat_image_plot_L/2-tgt[0,0,1].cpu(), sat_image_plot_L/2-tgt[0,0,0].cpu(), s=300, marker='^', facecolor='g', label='GT', edgecolors='white', zorder=1)\n",
    "        plt.quiver(grd_W+sat_image_plot_L/2-t[0,0,1].cpu(), sat_image_plot_L/2-t[0,0,0].cpu(), np.cos((90-yaw )/ 180 * np.pi), np.sin((90-yaw) / 180 * np.pi), facecolor='gold', linewidths=0.3, scale=25, width=0.006)\n",
    "        plt.scatter(grd_W+sat_image_plot_L/2-t[0,0,1].cpu(), sat_image_plot_L/2-t[0,0,0].cpu(), s=300, marker='*', facecolor='gold', label='Ours', edgecolors='white', zorder=1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b968f3a-f4e6-4b4a-813f-51fff8e91f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, weights = metric_coord_sat_B[batch_idx, sat_indices_sampled, :], metric_coord_grd_B[batch_idx, grd_indices_sampled, :], matches_row[batch_idx, sampled_matching_idx]\n",
    "metric_coord_grd_B = metric_coord_grd_B\n",
    "\n",
    "R, t, ok_rank = weighted_procrustes_2d(X, Y, use_weights=True, use_mask=True, w=weights)\n",
    "print('R', R)\n",
    "print('t', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ac90c-051b-421d-a0c4-3818bf2fb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(20, 40):\n",
    "    with torch.no_grad():\n",
    "        grd, sat, tgt, Rgt = test_set.__getitem__(idx)\n",
    "\n",
    "        grd, sat, tgt, Rgt = grd.unsqueeze(0).to(device), sat.unsqueeze(0).to(device), tgt.unsqueeze(0).to(device), Rgt.unsqueeze(0).to(device)\n",
    "\n",
    "        B, _, sat_size, _ = sat.shape\n",
    "        B, _, grd_H, grd_W = grd.shape\n",
    "        \n",
    "        grd_feature, sat_feature = desc_l2norm(shared_feature_extractor(grd)), desc_l2norm(shared_feature_extractor(sat))\n",
    "        _, _, grd_feature_H, grd_feature_W = grd_feature.size()\n",
    "        _, _, sat_feauture_size, _ = sat_feature.size()\n",
    "        \n",
    "        matching_score = torch.matmul(sat_feature.flatten(2).transpose(1, 2).contiguous(), grd_feature.flatten(2))\n",
    "        \n",
    "        matches_row = matching_score.flatten(1)\n",
    "        \n",
    "        top_indices = torch.argsort(matches_row, descending=True)[0,:num_samples_matches4vis]\n",
    "        sat_indices = torch.div(top_indices, grd_feature_H*grd_feature_W, rounding_mode='trunc')\n",
    "        grd_indices = top_indices % (grd_feature_H*grd_feature_W)\n",
    "\n",
    "        sat_h_indices = (torch.div(sat_indices, sat_feauture_size, rounding_mode='trunc')).cpu().numpy() * 14 + 7\n",
    "        sat_w_indices = (sat_indices % sat_feauture_size).cpu().numpy() * 14 + 7\n",
    "\n",
    "        grd_h_indices = (torch.div(grd_indices, grd_feature_W, rounding_mode='trunc')).cpu().numpy() * 14 + 7\n",
    "        grd_w_indices = (grd_indices % grd_feature_W).cpu().numpy() * 14 + 7\n",
    "\n",
    "        \n",
    "        grd_points = []\n",
    "        grd_bev_points = []\n",
    "        sat_points = []\n",
    "        for i in range(num_samples_matches4vis):\n",
    "            grd_points.append((grd_w_indices[i], grd_h_indices[i]))\n",
    "            sat_points.append((sat_w_indices[i]/sat_size*grd_H, sat_h_indices[i]/sat_size*grd_H))\n",
    "\n",
    "        \n",
    "        combined_image = np.ones((grd_H, grd_W+grd_H+10, 3)) \n",
    "        grd_to_show = grd[0].permute(1,2,0).cpu().detach().numpy()\n",
    "        sat_to_show = F.interpolate(sat, size=(grd_H, grd_H), mode='bicubic', align_corners=False)[0].permute(1,2,0).cpu().detach().numpy()\n",
    "    \n",
    "        # Place the images side-by-side on the canvas\n",
    "        combined_image[:, :grd_W, :] = grd_to_show\n",
    "        combined_image[:, 10+grd_W:, :] = sat_to_show\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(15,60))\n",
    "        ax.imshow(combined_image)\n",
    "        sample_index = 0\n",
    "        for (x1, y1), (x2, y2) in zip(grd_points, sat_points):\n",
    "            # Adjust x2 coordinate for the second image\n",
    "            x2_adjusted = x2 + 10+grd_W\n",
    "            # ax.plot([x1, x2_adjusted], [y1, y2], marker='o', markersize=4, color='lime', linestyle='-', linewidth=1, alpha=sampled_matching_scores[sample_index])\n",
    "            ax.plot([x1, x2_adjusted], [y1, y2], marker='o', markersize=2, color='lime', linestyle='-', linewidth=0.8, zorder=0)\n",
    "            sample_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44bb17-1060-4173-88ae-92fbc7187c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 21\n",
    "selected_grd_h = 25\n",
    "selected_grd_w = 0\n",
    "num_top_matches = 50\n",
    "\n",
    "with torch.no_grad():\n",
    "    grd, sat, tgt, Rgt = test_set.__getitem__(idx)\n",
    "    # sat = torch.flip(sat, dims=[2])\n",
    "    \n",
    "    grd, sat, tgt, Rgt = grd.unsqueeze(0).to(device), sat.unsqueeze(0).to(device), tgt.unsqueeze(0).to(device), Rgt.unsqueeze(0).to(device)\n",
    "    \n",
    "    B, _, sat_size, _ = sat.shape\n",
    "    B, _, grd_H, grd_W = grd.shape\n",
    "    \n",
    "    grd_feature, sat_feature = desc_l2norm(shared_feature_extractor(grd)), desc_l2norm(shared_feature_extractor(sat))\n",
    "    _, _, grd_feature_H, grd_feature_W = grd_feature.size()\n",
    "    _, _, sat_feauture_size, _ = sat_feature.size()\n",
    "\n",
    "    matching_score = torch.matmul(sat_feature.flatten(2).transpose(1, 2).contiguous(), grd_feature.flatten(2))\n",
    "    print(matching_score.shape)\n",
    "    \n",
    "    for selected_grd_w in range(0,85,7):\n",
    "        # select a point in the ground view\n",
    "        seletec_matching_col = selected_grd_h*grd_feature_W + selected_grd_w\n",
    "        sat_scores = matching_score[0,:,seletec_matching_col]\n",
    "        \n",
    "        \n",
    "        top_indices = torch.argsort(sat_scores, descending=True)[:num_top_matches]\n",
    "        \n",
    "        sat_h_indices = (torch.div(top_indices, sat_feauture_size, rounding_mode='trunc')).cpu().numpy() * 14 + 7\n",
    "        sat_w_indices = (top_indices % sat_feauture_size).cpu().numpy() * 14 + 7\n",
    "\n",
    "\n",
    "        grd_h_indices = selected_grd_h * 14 + 7\n",
    "        grd_w_indices = selected_grd_w * 14 + 7\n",
    "    \n",
    "        grd_points = []\n",
    "        sat_points = []\n",
    "        for i in range(num_top_matches):\n",
    "            grd_points.append((grd_w_indices, grd_h_indices))\n",
    "            sat_points.append((sat_w_indices[i]/sat_size*grd_H, sat_h_indices[i]/sat_size*grd_H))\n",
    "        grd_to_show = grd[0].permute(1,2,0).cpu().detach().numpy()\n",
    "        sat_to_show = F.interpolate(sat, size=(grd_H, grd_H), mode='bicubic', align_corners=False)[0].permute(1,2,0).cpu().detach().numpy()\n",
    "    \n",
    "    \n",
    "        # Create a new blank canvas for both images\n",
    "        combined_image = np.ones((grd_H, grd_W+grd_H+10, 3)) \n",
    "        \n",
    "        # Place the images side-by-side on the canvas\n",
    "        combined_image[:, :grd_W, :] = grd_to_show\n",
    "        combined_image[:, 10+grd_W:, :] = sat_to_show\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(10,30))\n",
    "        ax.imshow(combined_image)\n",
    "        for (x1, y1), (x2, y2) in zip(grd_points, sat_points):\n",
    "            # Adjust x2 coordinate for the second image\n",
    "            x2_adjusted = x2 + 10+grd_W\n",
    "            ax.plot([x1, x2_adjusted], [y1, y2], marker='o', markersize=2, color='lime', linestyle='-', linewidth=0.8, zorder=0)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a2151-4bd5-47ed-94dd-c8b8c8f36836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
